{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-3_YUfb6hpr"
      },
      "source": [
        "# **A Multimodal Sentiment Analysis Pipeline Incorporating Valence Detection and Topological Data Analysis - Model Prototype**\n",
        "---\n",
        "Authors/Researchers:\n",
        "1. Alverio, Franz Tovie G.\n",
        "2. Almarinez, Lucky Richmon C.\n",
        "3. Jamilano, Kyla Celine L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KY3Y0EqRU-F"
      },
      "source": [
        "# Prototype\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et0G9hZc4KMz"
      },
      "source": [
        "## **Stage 0: Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installation**:"
      ],
      "metadata": {
        "id": "qtvsRhI5qDTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install missingpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMzACfB4qAh5",
        "outputId": "762c100c-cd9d-4e8a-c511-f1e6b8517189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: missingpy in /usr/local/lib/python3.12/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Libraries**:"
      ],
      "metadata": {
        "id": "Cz_Apmbfq3Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#supression warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "u_8tuFwA1Gps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iJgCVCX4Jl8",
        "outputId": "a7f9ffe7-2469-4b6a-9e46-7bfd561f0b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Sentence-BERT and transformers available.\n",
            "Using device: cpu\n",
            "Default ResNet: resnet50\n",
            "Default BERT: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "# Cell 6iJgCVCX4Jl8: Initial Library Imports and Constants\n",
        "\n",
        "# --- Standard Libraries ---\n",
        "import os\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import shutil # Added shutil for find_metadata\n",
        "\n",
        "# --- Core Third-Party Libraries ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import random # Added random for create_audio_subset\n",
        "\n",
        "# --- PyTorch / Vision ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision # Ensure torchvision is imported here\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- Video ---\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# --- Audio ---\n",
        "import librosa\n",
        "\n",
        "# --- Text Encoders ---\n",
        "_HAS_SBERT, _HAS_TRANSFORMERS = False, False\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    _HAS_SBERT, _HAS_TRANSFORMERS = True, True\n",
        "    print(\"✓ Sentence-BERT and transformers available.\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModel\n",
        "        _HAS_TRANSFORMERS = True\n",
        "        print(\"✓ transformers available (fallback, no Sentence-BERT).\")\n",
        "    except ImportError:\n",
        "        print(\"✗ No text encoder libraries found. Install with:\")\n",
        "        print(\"  pip install sentence-transformers transformers\")\n",
        "\n",
        "# --- Imputation ---\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Sklearn (Classifiers, Metrics, Preprocessing, Model Selection) ---\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    mean_absolute_error, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "\n",
        "# --- Explainability ---\n",
        "# import shap # Commented out for now as it's not used in the provided code\n",
        "\n",
        "# --- Constants ---\n",
        "DEFAULT_RESNET = \"resnet50\"\n",
        "DEFAULT_BERT = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Default ResNet: {DEFAULT_RESNET}\")\n",
        "print(f\"Default BERT: {DEFAULT_BERT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentiment Lexicon**:"
      ],
      "metadata": {
        "id": "Za82e8vjpaiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiment lexicon here as it's a core constant\n",
        "sentiment_lexicon = {\n",
        "    'happy': 1, 'joy': 1, 'glad': 1, 'positive': 1,\n",
        "    'sad': -1, 'unhappy': -1, 'grief': -1, 'negative': -1,\n",
        "    'angry': -1, 'fear': -1, 'disgust': -1, 'neutral': 0\n",
        "}\n",
        "print(\"✓ Sentiment lexicon initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5yt_lRQpZdz",
        "outputId": "4ba84a2f-80bf-44d2-a240-4563d87cab7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Sentiment lexicon initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lw-Q_Mr7tXY"
      },
      "source": [
        "## **Stage 1: Multimodal Inputs**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initial Import Dataset Functions:**"
      ],
      "metadata": {
        "id": "Z467sLh8tlmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89StvGbdyzx8"
      },
      "outputs": [],
      "source": [
        "def clone_repo(path, url, name):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Cloning {name} into {path}...\")\n",
        "        !git clone {url} {path}\n",
        "    else:\n",
        "        print(f\"{name} already exists at {path}.\")\n",
        "\n",
        "def download_csvs(base_url, dest_dir, files):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    for f in files:\n",
        "        path = os.path.join(dest_dir, f)\n",
        "        if not os.path.exists(path):\n",
        "            try:\n",
        "                urllib.request.urlretrieve(f\"{base_url}{f}\", path)\n",
        "                print(f\"✓ Downloaded {f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error downloading {f}: {e}\")\n",
        "        else:\n",
        "            print(f\"{f} already exists.\")\n",
        "\n",
        "def load_csvs(path, files):\n",
        "    try:\n",
        "        dfs = [pd.read_csv(os.path.join(path, f)) for f in files]\n",
        "        print(\"✓ MELD loaded:\", [df.shape for df in dfs])\n",
        "        return dfs\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading MELD CSVs: {e}\")\n",
        "        return [None]*len(files)\n",
        "\n",
        "def find_metadata(base_path, expected):\n",
        "    # Ensure shutil is imported globally or here if needed\n",
        "    # import shutil\n",
        "    for root, _, files in os.walk(base_path):\n",
        "        for f in files:\n",
        "            if f.endswith(\".csv\"):\n",
        "                src = os.path.join(root, f)\n",
        "                if src != expected:\n",
        "                    # Check if shutil is available before using it\n",
        "                    if 'shutil' in globals():\n",
        "                        shutil.copy(src, expected)\n",
        "                    else:\n",
        "                        print(\"Warning: shutil not imported. Cannot copy metadata file.\")\n",
        "                return expected\n",
        "    return None\n",
        "\n",
        "def create_audio_subset(audio_dir, subset_dir, n=50):\n",
        "    # Ensure random and shutil are imported globally or here if needed\n",
        "    # import random, shutil\n",
        "    if os.path.exists(audio_dir):\n",
        "        os.makedirs(subset_dir, exist_ok=True)\n",
        "        files = [f for f in os.listdir(audio_dir) if f.endswith(\".mp3\")]\n",
        "        subset = random.sample(files, min(n, len(files)))\n",
        "        for f in subset:\n",
        "             # Check if shutil is available before using it\n",
        "             if 'shutil' in globals():\n",
        "                 shutil.copy(os.path.join(audio_dir, f), os.path.join(subset_dir, f))\n",
        "             else:\n",
        "                 print(\"Warning: shutil not imported. Cannot copy audio files.\")\n",
        "        print(f\"✓ Copied {len(subset)} CREMA-D audio files.\")\n",
        "    else:\n",
        "        print(f\"✗ No audio directory at {audio_dir}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CREMA-D:**"
      ],
      "metadata": {
        "id": "SGR8s32js_nu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2VJfEgOYCww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREMA-D ---\n",
        "crema_repo = \"/content/CREMA-D\"\n",
        "clone_repo(crema_repo, \"https://gitlab.com/cs-cooper-lab/crema-d-mirror.git\", \"CREMA-D\")\n",
        "\n",
        "crema_meta_path = os.path.join(crema_repo, \"processedResults\", \"summaryTable.csv\")\n",
        "os.makedirs(os.path.dirname(crema_meta_path), exist_ok=True)\n",
        "meta_file = find_metadata(crema_repo, crema_meta_path)\n",
        "\n",
        "crema_meta = None\n",
        "if meta_file and os.path.exists(meta_file):\n",
        "    try:\n",
        "        crema_meta = pd.read_csv(meta_file)\n",
        "        print(\"✓ CREMA-D metadata:\", crema_meta.shape)\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading CREMA-D metadata: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFMUFtyXs-vv",
        "outputId": "256ffcf4-0ac6-48e5-880a-9008cc4302c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning CREMA-D into /content/CREMA-D...\n",
            "Cloning into '/content/CREMA-D'...\n",
            "remote: Enumerating objects: 22366, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 22366 (delta 5), reused 0 (delta 0), pack-reused 22351 (from 1)\u001b[K\n",
            "Receiving objects: 100% (22366/22366), 13.09 MiB | 10.56 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Updating files: 100% (22341/22341), done.\n",
            "Filtering content: 100% (22326/22326), 3.42 GiB | 3.01 MiB/s, done.\n",
            "✓ CREMA-D metadata: (7442, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStage 2: Data Import complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wROMA07p2AIz",
        "outputId": "f5798789-7d68-4e87-8ad9-8ca03f8b620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 2: Data Import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crema_audio_dir = os.path.join(crema_repo, \"AudioMP3\")\n",
        "print(\"Audio files found:\", len(os.listdir(crema_audio_dir)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h1SWHC_4XhP",
        "outputId": "9222abff-0d6c-40d0-d43d-85b3259d1c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio files found: 7442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGBFMTmpJX8b"
      },
      "source": [
        "## **Stage 2: Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Video Preprocessing (Franz)**"
      ],
      "metadata": {
        "id": "_DYrU-oxsuvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video(path: str, frame_time: float = 1.0, sr: int = 22050):\n",
        "    \"\"\"\n",
        "    Extract audio + frame from video (.mp4).\n",
        "    Returns: (audio_data, image_tensor) or (None, None)\n",
        "    \"\"\"\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None, None\n",
        "    audio_data, image_tensor = None, None\n",
        "    try:\n",
        "        clip = VideoFileClip(path)\n",
        "        # audio\n",
        "        # Note: Saving to a temp file might be necessary if librosa.load\n",
        "        # cannot directly read from the clip's audio object.\n",
        "        # For simplicity, assuming direct processing or a temp file approach is handled elsewhere if needed.\n",
        "        # A common pattern is to extract audio to a .wav first.\n",
        "        audio_temp_path = path.replace(\".mp4\", \"_temp_audio.wav\")\n",
        "        try:\n",
        "             clip.audio.write_audiofile(audio_temp_path, fps=sr, verbose=False, logger=None)\n",
        "             audio_data_sr = preprocess_audio(audio_temp_path, sr)\n",
        "             if audio_data_sr:\n",
        "                 audio_data, sr = audio_data_sr\n",
        "             os.remove(audio_temp_path) # Clean up temp file\n",
        "        except Exception as e:\n",
        "             print(f\"✗ Video audio extraction error {path}: {e}\")\n",
        "             audio_data = None\n",
        "\n",
        "        # frame\n",
        "        try:\n",
        "            frame = clip.get_frame(frame_time)\n",
        "            image_tensor = _img_transform(Image.fromarray(frame))\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Video frame extraction error {path}: {e}\")\n",
        "            image_tensor = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Video error {path}: {e}\")\n",
        "    return audio_data, image_tensor"
      ],
      "metadata": {
        "id": "cpUledtusuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Text Preprocessing (Franz)**"
      ],
      "metadata": {
        "id": "HcZH7jCNsUkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Text ---\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Clean text: lowercase, remove URLs + special chars.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "jsU__pkhsU-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Image Preprocessing (Kyla)**"
      ],
      "metadata": {
        "id": "M4AKaoNksegX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Image ---\n",
        "# Ensure _img_transform is defined globally or within this cell if it's only used here\n",
        "_img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "def preprocess_image(path: str):\n",
        "    \"\"\"Load image → 224x224 tensor (normalized).\"\"\"\n",
        "    if not path or not os.path.exists(path):\n",
        "        # print(f\"✗ Image path missing or not found: {path}\") # Suppress frequent messages\n",
        "        return None\n",
        "    try:\n",
        "        return _img_transform(Image.open(path).convert(\"RGB\"))\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Image error {path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "eJZkuzOOslyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Audio Preprocessing (Lucky)**"
      ],
      "metadata": {
        "id": "svd7VW_Nsnqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Audio ---\n",
        "def preprocess_audio(path: str, sr: int = 22050):\n",
        "    \"\"\"Load audio (.wav/.mp3) → waveform + sample rate.\"\"\"\n",
        "    if not path or not os.path.exists(path):\n",
        "        # print(f\"✗ Audio path missing or not found: {path}\") # Suppress frequent messages\n",
        "        return None\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=sr)\n",
        "        return y, sr\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Audio error {path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "PsvU5cAXssHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXIbJiuLJcAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed80c816-97cc-4f8d-dba7-1092d3921b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 3: Preprocessing functions defined.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStage 3: Preprocessing functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3WGSyzCRupd"
      },
      "source": [
        "## **Stage 3: Feature Extraction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Matrix Construction"
      ],
      "metadata": {
        "id": "-2LBZGYBoe8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Matrix Valence (Schema Definition)"
      ],
      "metadata": {
        "id": "VS04ud7HooJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_feature_schema(text_dim=768, img_dim=2048, aud_dim=13, lex_dim=1):\n",
        "    \"\"\"\n",
        "    Define the schema (column names) for multimodal features\n",
        "    without performing any extraction.\n",
        "    Returns a list of feature column names.\n",
        "    \"\"\"\n",
        "    schema = []\n",
        "\n",
        "    # Text\n",
        "    schema += [f\"text_feat_{i}\" for i in range(text_dim)]\n",
        "    # Image\n",
        "    schema += [f\"img_feat_{i}\" for i in range(img_dim)]\n",
        "    # Audio\n",
        "    schema += [f\"aud_feat_{i}\" for i in range(aud_dim)]\n",
        "    # Lexicon\n",
        "    schema += [f\"lexicon_feat_{i}\" for i in range(lex_dim)]\n",
        "\n",
        "    # Metadata / labels\n",
        "    schema += [\"dialogue_id\", \"utterance_id\", \"speaker\",\n",
        "               \"emotion_label\", \"valence_label\", \"valence_label_numeric\",\n",
        "               \"feature_extraction_successful\"]\n",
        "\n",
        "    return schema"
      ],
      "metadata": {
        "id": "cMYPdOGkrX53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Matrix Valence (Proper)"
      ],
      "metadata": {
        "id": "MPNwJMQurdc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_row(row, text_ext, img_ext, aud_ext,\n",
        "                              image_dir=None, audio_dir=None, sentiment_lexicon=None):\n",
        "    \"\"\"\n",
        "    Extract multimodal features for a single row.\n",
        "    Uses schema from define_feature_schema but fills with actual values or NaN.\n",
        "    \"\"\"\n",
        "    row_feats = {}\n",
        "    feature_extraction_successful = True\n",
        "\n",
        "    # --- Text features ---\n",
        "    text_dim = getattr(text_ext, 'dim', 768)\n",
        "    if 'text' in row and pd.notna(row['text']):\n",
        "        try:\n",
        "            text_features_raw = text_ext.extract(row['text'])\n",
        "            if isinstance(text_features_raw, np.ndarray):\n",
        "                row_feats.update({f\"text_feat_{i}\": val for i, val in enumerate(text_features_raw)})\n",
        "            elif isinstance(text_features_raw, dict):\n",
        "                row_feats.update(text_features_raw)\n",
        "            else:\n",
        "                row_feats.update({f\"text_feat_{i}\": np.nan for i in range(text_dim)})\n",
        "                feature_extraction_successful = False\n",
        "        except:\n",
        "            row_feats.update({f\"text_feat_{i}\": np.nan for i in range(text_dim)})\n",
        "            feature_extraction_successful = False\n",
        "    else:\n",
        "        row_feats.update({f\"text_feat_{i}\": np.nan for i in range(text_dim)})\n",
        "\n",
        "    # --- Image features ---\n",
        "    img_dim = getattr(img_ext, 'dim', 2048)\n",
        "    img_col = row.get('image_filename') or row.get('image_path')\n",
        "    if image_dir and img_col and os.path.exists(os.path.join(image_dir, img_col)):\n",
        "        try:\n",
        "            img_path = os.path.join(image_dir, img_col)\n",
        "            preprocessed_img = preprocess_image(img_path)\n",
        "            if preprocessed_img is not None:\n",
        "                img_features_raw = img_ext.extract(preprocessed_img)\n",
        "                if isinstance(img_features_raw, np.ndarray):\n",
        "                    row_feats.update({f\"img_feat_{i}\": val for i, val in enumerate(img_features_raw)})\n",
        "                elif isinstance(img_features_raw, dict):\n",
        "                    row_feats.update(img_features_raw)\n",
        "                else:\n",
        "                    row_feats.update({f\"img_feat_{i}\": np.nan for i in range(img_dim)})\n",
        "                    feature_extraction_successful = False\n",
        "            else:\n",
        "                row_feats.update({f\"img_feat_{i}\": np.nan for i in range(img_dim)})\n",
        "                feature_extraction_successful = False\n",
        "        except:\n",
        "            row_feats.update({f\"img_feat_{i}\": np.nan for i in range(img_dim)})\n",
        "            feature_extraction_successful = False\n",
        "    else:\n",
        "        row_feats.update({f\"img_feat_{i}\": np.nan for i in range(img_dim)})\n",
        "\n",
        "    # --- Audio features ---\n",
        "    aud_dim = getattr(aud_ext, 'n_mfcc', 13)\n",
        "    aud_col = row.get('audio_filename') or row.get('audio_path')\n",
        "    if audio_dir and aud_col and os.path.exists(os.path.join(audio_dir, aud_col)):\n",
        "        try:\n",
        "            aud_path = os.path.join(audio_dir, aud_col)\n",
        "            audio_data_sr = preprocess_audio(aud_path)\n",
        "            if audio_data_sr:\n",
        "                audio_data, sr = audio_data_sr\n",
        "                aud_features_raw = aud_ext.extract(audio_data, sr)\n",
        "                if isinstance(aud_features_raw, np.ndarray):\n",
        "                    row_feats.update({f\"aud_feat_{i}\": val for i, val in enumerate(aud_features_raw)})\n",
        "                elif isinstance(aud_features_raw, dict):\n",
        "                    row_feats.update(aud_features_raw)\n",
        "                else:\n",
        "                    row_feats.update({f\"aud_feat_{i}\": np.nan for i in range(aud_dim)})\n",
        "                    feature_extraction_successful = False\n",
        "            else:\n",
        "                row_feats.update({f\"aud_feat_{i}\": np.nan for i in range(aud_dim)})\n",
        "                feature_extraction_successful = False\n",
        "        except:\n",
        "            row_feats.update({f\"aud_feat_{i}\": np.nan for i in range(aud_dim)})\n",
        "            feature_extraction_successful = False\n",
        "    else:\n",
        "        row_feats.update({f\"aud_feat_{i}\": np.nan for i in range(aud_dim)})\n",
        "\n",
        "    # --- Lexicon features ---\n",
        "    if sentiment_lexicon and 'text' in row and pd.notna(row['text']):\n",
        "        try:\n",
        "            lex_raw = get_lexicon_sentiment_features(row['text'], sentiment_lexicon)\n",
        "            if isinstance(lex_raw, np.ndarray):\n",
        "                row_feats.update({f\"lexicon_feat_{i}\": val for i, val in enumerate(lex_raw)})\n",
        "            elif isinstance(lex_raw, dict):\n",
        "                row_feats.update(lex_raw)\n",
        "            else:\n",
        "                row_feats.update({\"lexicon_feat_0\": np.nan})\n",
        "                feature_extraction_successful = False\n",
        "        except:\n",
        "            row_feats.update({\"lexicon_feat_0\": np.nan})\n",
        "            feature_extraction_successful = False\n",
        "    else:\n",
        "        row_feats.update({\"lexicon_feat_0\": np.nan})\n",
        "\n",
        "    # --- Metadata ---\n",
        "    row_feats[\"dialogue_id\"] = row.get(\"Dialogue_ID\")\n",
        "    row_feats[\"utterance_id\"] = row.get(\"Utterance_ID\")\n",
        "    row_feats[\"speaker\"] = row.get(\"Speaker\")\n",
        "    row_feats[\"emotion_label\"] = row.get(\"label\")\n",
        "    row_feats[\"valence_label\"] = row.get(\"valence_label\")\n",
        "\n",
        "    valence_map = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}\n",
        "    row_feats[\"valence_label_numeric\"] = valence_map.get(row_feats[\"valence_label\"], np.nan)\n",
        "\n",
        "    row_feats[\"feature_extraction_successful\"] = feature_extraction_successful\n",
        "\n",
        "    return row_feats"
      ],
      "metadata": {
        "id": "Jq9Lnv48oumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Matrix Valence (Orchestration)"
      ],
      "metadata": {
        "id": "xIuQ5P0er5MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_matrix_valence(df, text_ext, img_ext, aud_ext,\n",
        "                                 image_dir=None, audio_dir=None, sentiment_lexicon=None):\n",
        "    \"\"\"\n",
        "    Build the feature matrix by applying extract_features_from_row\n",
        "    to each row in the dataframe.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        feats = extract_features_from_row(row, text_ext, img_ext, aud_ext,\n",
        "                                          image_dir=image_dir,\n",
        "                                          audio_dir=audio_dir,\n",
        "                                          sentiment_lexicon=sentiment_lexicon)\n",
        "        features.append(feats)\n",
        "\n",
        "    features_df = pd.DataFrame(features, columns=define_feature_schema())\n",
        "    print(\"Feature matrix built:\", features_df.shape)\n",
        "    return features_df"
      ],
      "metadata": {
        "id": "upDCF6Fprxwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Lexicon Extraction\n"
      ],
      "metadata": {
        "id": "oZXIxSS0hi1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lexicon_sentiment_features(text, lexicon=sentiment_lexicon):\n",
        "    \"\"\"Return mean sentiment score based on a lexicon.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return np.array([0.0])\n",
        "\n",
        "    words = preprocess_text(text).split()\n",
        "    scores = [lexicon.get(word, 0) for word in words]\n",
        "    return np.array([np.mean(scores)]) if scores else np.array([0.0])\n"
      ],
      "metadata": {
        "id": "OsM4DOb2iPUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio Feature Extractor"
      ],
      "metadata": {
        "id": "HLHPM8p2n-n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A1D0oG7ooE4d: Stage 4 - Valence-specific Audio Feature Extractor\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import torch.nn as nn, numpy as np, librosa\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "class AudioFeatureExtractorValence(nn.Module):\n",
        "    \"\"\"Extract MFCC features for valence analysis.\"\"\"\n",
        "    def __init__(self, n_mfcc=13, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.device = device # Device might not be strictly needed for librosa, but keep for consistency\n",
        "\n",
        "    # Modified extract method to accept audio_data and sr\n",
        "    def extract(self, audio_data, sr):\n",
        "        # Removed the redundant local imports as numpy and librosa are imported globally\n",
        "        # import numpy as np\n",
        "        # import librosa\n",
        "\n",
        "        if audio_data is None or sr is None:\n",
        "            # Return a dictionary with named features\n",
        "            return {f'aud_feat_{i}': np.nan for i in range(self.n_mfcc)}\n",
        "        try:\n",
        "            # Ensure audio_data is a numpy array with float32 dtype\n",
        "            audio_data = np.asarray(audio_data, dtype=np.float32)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=audio_data, # Use the provided audio_data\n",
        "                sr=sr,         # Use the provided sample rate\n",
        "                n_mfcc=self.n_mfcc,\n",
        "                n_fft=2048,\n",
        "                hop_length=512\n",
        "            )\n",
        "            # Average over time and convert to dictionary with named features\n",
        "            features_array = mfccs.mean(axis=1)\n",
        "            return {f'aud_feat_{i}': features_array[i] for i in range(self.n_mfcc)}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[AudioFeatureExtractorValence] Error: {e}\")\n",
        "            # Return placeholder dictionary on error\n",
        "            return {f'aud_feat_{i}': np.nan for i in range(self.n_mfcc)}\n",
        "\n",
        "print(\"\\nStage 4: Valence-specific Audio Feature Extractor defined.\")"
      ],
      "metadata": {
        "id": "A1D0oG7ooE4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a33142-f99e-49e9-9e7f-7e48f0f0aafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 4: Valence-specific Audio Feature Extractor defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Feature Extractor"
      ],
      "metadata": {
        "id": "_9LmRPehoLUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell glbWaK4voQmd: Stage 4 - Base Feature Extractors (Text)\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import torch, torch.nn as nn, numpy as np\n",
        "# from transformers import AutoTokenizer, AutoModel # Ensure transformers are imported\n",
        "\n",
        "class TextFeatureExtractor:\n",
        "    def __init__(self, model_name=DEFAULT_BERT, device=DEVICE):\n",
        "        # Check if necessary libraries are available globally\n",
        "        if '_HAS_SBERT' not in globals() or '_HAS_TRANSFORMERS' not in globals() or not (_HAS_SBERT or _HAS_TRANSFORMERS):\n",
        "            raise ImportError(\"Neither Sentence-BERT nor transformers available globally.\")\n",
        "\n",
        "        self.device = device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
        "        # Store dimension\n",
        "        self.dim = self.model.config.hidden_size\n",
        "\n",
        "    def extract(self, text):\n",
        "        if not text:\n",
        "            # Return a dictionary with named features\n",
        "            return {f'text_feat_{i}': 0.0 for i in range(self.dim)}\n",
        "        try:\n",
        "            # Ensure torch is available\n",
        "            if 'torch' not in globals():\n",
        "                 import torch\n",
        "            encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
        "            with torch.no_grad():\n",
        "                output = self.model(**encoded)\n",
        "            # Convert numpy array output to a dictionary with named features\n",
        "            features_array = output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            return {f'text_feat_{i}': features_array[i] for i in range(self.dim)}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[TextFeatureExtractor] Error: {e}\")\n",
        "            # Return placeholder dictionary on error\n",
        "            return {f'text_feat_{i}': np.nan for i in range(self.dim)}\n",
        "\n",
        "print(\"\\nStage 4: Text Feature Extractor defined.\")"
      ],
      "metadata": {
        "id": "glbWaK4voQmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ecf259c-44f8-4a85-ac3e-6b245de4c0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 4: Text Feature Extractor defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Feature Extractor"
      ],
      "metadata": {
        "id": "gfZxqMxQoTMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell BY4OgFkMoR_V: Stage 4 - Base Feature Extractors (Image)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "class ImageFeatureExtractor(nn.Module):\n",
        "    def __init__(self, model_name=DEFAULT_RESNET, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        try:\n",
        "            # Ensure torchvision is imported globally or within the class\n",
        "            if 'torchvision' not in globals():\n",
        "                 import torchvision\n",
        "            backbone = getattr(torchvision.models, model_name)(pretrained=True)\n",
        "        except AttributeError:\n",
        "            raise ValueError(f\"Unsupported image model: {model_name}\")\n",
        "        # Remove classifier head\n",
        "        self.model = nn.Sequential(*list(backbone.children())[:-1]).to(device).eval()\n",
        "        # Determine output dimension by passing a dummy tensor\n",
        "        try:\n",
        "            if 'torch' not in globals():\n",
        "                 import torch\n",
        "            with torch.no_grad():\n",
        "                dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "                self.dim = self.model(dummy_input).squeeze().shape[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not determine image feature dimension: {e}\")\n",
        "            self.dim = 2048 # Default for ResNet50/101/152 features before FC\n",
        "\n",
        "\n",
        "    def extract(self, image_tensor):\n",
        "        # Ensure torch and numpy are available\n",
        "        if 'torch' not in globals():\n",
        "             import torch\n",
        "        if 'np' not in globals():\n",
        "             import numpy as np\n",
        "\n",
        "        if image_tensor is None:\n",
        "            # Return a dictionary with named features\n",
        "            return {f'img_feat_{i}': np.nan for i in range(self.dim)}\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # Add batch dimension if needed and move to device\n",
        "                if image_tensor.ndim == 3:\n",
        "                    image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
        "                elif image_tensor.device != self.device:\n",
        "                    image_tensor = image_tensor.to(self.device)\n",
        "\n",
        "                features_tensor = self.model(image_tensor).squeeze()\n",
        "                # Convert numpy array output to a dictionary with named features\n",
        "                features_array = features_tensor.cpu().numpy()\n",
        "                return {f'img_feat_{i}': features_array[i] for i in range(self.dim)}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ImageFeatureExtractor] Error: {e}\")\n",
        "            # Return placeholder dictionary on error\n",
        "            return {f'img_feat_{i}': np.nan for i in range(self.dim)}\n",
        "\n",
        "print(\"\\nStage 4: Image Feature Extractor defined.\")"
      ],
      "metadata": {
        "id": "BY4OgFkMoR_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e477a4-7e5c-48ca-f17f-02c89a53869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 4: Image Feature Extractor defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction (Overall)"
      ],
      "metadata": {
        "id": "gXXCXiyJA55a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "fF929Tv8BITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your feature extractors and preprocessors are already defined:\n",
        "# TextFeatureExtractor, AudioFeatureExtractorValence, ImageFeatureExtractor\n",
        "# preprocess_audio, preprocess_video, preprocess_image\n",
        "# get_lexicon_sentiment_features, define_feature_schema, extract_features_from_row\n",
        "\n",
        "# --- Initialize extractors ---\n",
        "text_extractor = TextFeatureExtractor(model_name=\"bert-base-uncased\", device='cpu')\n",
        "audio_extractor = AudioFeatureExtractorValence(n_mfcc=13, device='cpu')\n",
        "image_extractor = ImageFeatureExtractor(model_name=\"resnet50\", device='cpu')\n",
        "\n",
        "# Assume sentiment_lexicon is a dictionary {word: score}\n",
        "lexicon = sentiment_lexicon\n",
        "\n",
        "# Path to CREMA-D videos and metadata\n",
        "crema_video_dir = \"/content/CREMA-D/VideoClips\"\n",
        "crema_meta_path = \"/content/CREMA-D/processedResults/summaryTable.csv\"\n",
        "crema_meta = pd.read_csv(crema_meta_path)\n",
        "\n",
        "# Optional: if audio files exist separately\n",
        "crema_audio_dir = \"/content/CREMA-D/AudioClips\"\n",
        "\n",
        "# --- Build feature matrix ---\n",
        "features_list = []\n",
        "\n",
        "for idx, row in tqdm(crema_meta.iterrows(), total=len(crema_meta), desc=\"Processing CREMA-D\"):\n",
        "    row_dict = {}\n",
        "\n",
        "    # --- 1. Text ---\n",
        "    text_features = text_extractor.extract(row.get(\"text\", \"\"))\n",
        "\n",
        "    # --- 2. Lexicon ---\n",
        "    lex_raw = get_lexicon_sentiment_features(row.get(\"text\", \"\"), lexicon)\n",
        "    lex_dict = {f\"lexicon_feat_0\": lex_raw[0]}\n",
        "\n",
        "    # --- 3. Video & Image ---\n",
        "    video_path = row.get(\"video_path\") or os.path.join(crema_video_dir, row[\"Filename\"])\n",
        "    audio_data, frame_tensor = preprocess_video(video_path, frame_time=1.0, sr=22050)\n",
        "\n",
        "    # Image features from frame\n",
        "    img_features = image_extractor.extract(frame_tensor)\n",
        "\n",
        "    # Audio features from extracted waveform\n",
        "    aud_features = audio_extractor.extract(audio_data, 22050)\n",
        "\n",
        "    # --- 4. Combine features ---\n",
        "    row_dict.update(text_features)\n",
        "    row_dict.update(img_features)\n",
        "    row_dict.update(aud_features)\n",
        "    row_dict.update(lex_dict)\n",
        "\n",
        "    # --- 5. Metadata ---\n",
        "    row_dict[\"dialogue_id\"] = row.get(\"Dialogue_ID\")\n",
        "    row_dict[\"utterance_id\"] = row.get(\"Utterance_ID\")\n",
        "    row_dict[\"speaker\"] = row.get(\"Speaker\")\n",
        "    row_dict[\"emotion_label\"] = row.get(\"label\")\n",
        "    row_dict[\"valence_label\"] = row.get(\"valence_label\")\n",
        "    valence_map = {\"positive\":1, \"negative\":-1, \"neutral\":0}\n",
        "    row_dict[\"valence_label_numeric\"] = valence_map.get(row_dict[\"valence_label\"], np.nan)\n",
        "    row_dict[\"feature_extraction_successful\"] = True\n",
        "\n",
        "    features_list.append(row_dict)\n",
        "\n",
        "# --- 6. Convert to DataFrame ---\n",
        "features_df = pd.DataFrame(features_list, columns=define_feature_schema())\n",
        "print(\"CREMA-D feature matrix shape:\", features_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "B8q1_DwwA5RS",
        "outputId": "91fd56fa-f5bc-4e0b-b716-cf3dacf04c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'torchvision' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-264748769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtext_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0maudio_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioFeatureExtractorValence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimage_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assume sentiment_lexicon is a dictionary {word: score}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2864926130.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'torchvision'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                  \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsupported image model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'torchvision' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFvF8tMxR6nG"
      },
      "source": [
        "## **Stage 4: Fusion & Robustness**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sOQBx3dR4NY"
      },
      "source": [
        "### Imputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3acc1de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc54c2ba-ad68-4a10-8ed6-66fcc19c6d4a"
      },
      "source": [
        "# Define the main training pipeline function\n",
        "# This function encapsulates the steps for feature extraction, imputation, training, and evaluation.\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1, but included for clarity)\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt # Added for plotting\n",
        "import seaborn as sns # Added for plotting\n",
        "\n",
        "\n",
        "def run_training_pipeline(args):\n",
        "    \"\"\"\n",
        "    Runs the end-to-end training and evaluation pipeline.\n",
        "\n",
        "    Args:\n",
        "        args (ConfigArgs): An object containing configuration parameters.\n",
        "                             Expected attributes: manifest, out_dir, bert_model,\n",
        "                             tda, trees, max_depth.\n",
        "    Returns:\n",
        "        dict: A dictionary containing the trained model package (model, scaler,\n",
        "                imputer, label_encoder) or None if training failed.\n",
        "    \"\"\"\n",
        "    # Declare global variables at the beginning of the function\n",
        "    global evaluation_results_valence\n",
        "\n",
        "    print(\"\\n--- Running Training Pipeline ---\")\n",
        "\n",
        "    # --- 1. Load Data Manifest ---\n",
        "    if not os.path.exists(args.manifest):\n",
        "        print(f\"Error: Manifest file not found at {args.manifest}\")\n",
        "        return None\n",
        "    try:\n",
        "        manifest_df = pd.read_csv(args.manifest)\n",
        "        print(f\"Manifest loaded: {manifest_df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading manifest: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. Initialize Feature Extractors ---\n",
        "    print(\"\\nInitializing Feature Extractors...\")\n",
        "    try:\n",
        "        text_extractor = TextFeatureExtractor(model_name=args.bert_model)\n",
        "        image_extractor = ImageFeatureExtractor() # Using default resnet50\n",
        "        audio_extractor = AudioFeatureExtractorValence() # Using default n_mfcc=13\n",
        "\n",
        "        # Initialize TDA object if enabled and libraries are available - REMOVED\n",
        "        # persim_obj = None # REMOVED\n",
        "        # if args.tda and '_HAS_TDA' in globals() and _HAS_TDA: # REMOVED\n",
        "        #     # Assuming a default pixel size for PersImage for now # REMOVED\n",
        "        #     # This might need to be configurable or determined dynamically # REMOVED\n",
        "        #     persim_obj = PersImage(pixels=[20, 20], verbose=False) # Example pixels # REMOVED\n",
        "\n",
        "        print(\"Feature extractors initialized.\")\n",
        "    except ImportError as e:\n",
        "        print(f\"Error initializing feature extractors: {e}\")\n",
        "        print(\"Ensure necessary libraries are installed (transformers, torchvision, librosa).\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during extractor initialization: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # --- 3. Build Feature Matrix ---\n",
        "    print(\"\\nBuilding Feature Matrix...\")\n",
        "    try:\n",
        "        # Use the corrected build_feature_matrix_valence function\n",
        "        features_df = build_feature_matrix_valence(\n",
        "            manifest_df, text_extractor, image_extractor, audio_extractor,\n",
        "            image_dir=None, # Specify if you have image files\n",
        "            audio_dir=None, # Specify if you have audio files\n",
        "            sentiment_lexicon=sentiment_lexicon, # Use the global sentiment lexicon\n",
        "        )\n",
        "        print(f\"Feature matrix built: {features_df.shape}\")\n",
        "        # Check for successful feature extraction\n",
        "        if not features_df['feature_extraction_successful'].all():\n",
        "            print(\"Warning: Feature extraction failed for some rows. These rows will be excluded.\")\n",
        "            # Optionally filter out rows where feature extraction was not successful\n",
        "            features_df = features_df[features_df['feature_extraction_successful']].drop(columns=['feature_extraction_successful'])\n",
        "            print(f\"Feature matrix after filtering unsuccessful extractions: {features_df.shape}\")\n",
        "        else:\n",
        "            features_df = features_df.drop(columns=['feature_extraction_successful'])\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during feature matrix building: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Separate features (X) and labels (y)\n",
        "    # Identify feature columns (exclude labels and metadata)\n",
        "    # Assuming columns not in this list are features. Adjust if needed.\n",
        "    label_cols = ['emotion_label', 'valence_label', 'valence_label_numeric']\n",
        "    meta_cols = ['dialogue_id', 'utterance_id', 'speaker']\n",
        "    # Also exclude any potential TDA placeholder columns if TDA was requested but not available - REMOVED\n",
        "    # placeholder_tda_cols = [col for col in features_df.columns if col.startswith('tda_feat_') and features_df[col].isnull().all()] # REMOVED\n",
        "    # Combine all non-feature columns to exclude - MODIFIED\n",
        "    non_feature_cols = label_cols + meta_cols # placeholder_tda_cols removed\n",
        "    feature_cols = [col for col in features_df.columns if col not in non_feature_cols]\n",
        "\n",
        "    X = features_df[feature_cols].values\n",
        "    # Use the numeric valence label for the target variable 'y'\n",
        "    # Ensure 'valence_label_numeric' exists and select it\n",
        "    if 'valence_label_numeric' in features_df.columns:\n",
        "        y = features_df['valence_label_numeric'].values\n",
        "        # Keep the original string labels for potential later use (e.g., confusion matrix)\n",
        "        y_string_labels = features_df['valence_label'].values\n",
        "        emotion_string_labels = features_df['emotion_label'].values # Also keep emotion labels\n",
        "    else:\n",
        "        print(\"Error: 'valence_label_numeric' column not found in features_df.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Feature matrix shape (X): {X.shape}\")\n",
        "    print(f\"Label vector shape (y): {y.shape}\")\n",
        "    # print(f\"Sample X (first row): {X[0, :10]}\") # Debug print\n",
        "    # print(f\"Sample y (first 10): {y[:10]}\") # Debug print\n",
        "\n",
        "\n",
        "    # --- Handle Invalid Labels and Label Encoding ---\n",
        "    print(\"\\nHandling labels...\")\n",
        "    # Define valence_map here before it's used\n",
        "    valence_map = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}\n",
        "\n",
        "    # Use the numeric valence label for the target variable 'y'\n",
        "    # Filter out samples with missing or invalid numeric valence labels\n",
        "    # Invalid labels are defined by pd.isna or potentially specific numeric values if applicable\n",
        "    valid_label_mask = ~pd.isna(y)\n",
        "\n",
        "    print(f\"Total samples: {len(y)}\")\n",
        "    print(f\"Valid samples for training: {valid_label_mask.sum()}\")\n",
        "\n",
        "    # Apply the mask to X and y\n",
        "    X_valid = X[valid_label_mask]\n",
        "    y_valid = y[valid_label_mask]\n",
        "    y_string_labels_valid = y_string_labels[valid_label_mask] # Keep string labels for valid data\n",
        "\n",
        "    # For valence classification, the numeric labels (1, -1, 0) are already suitable.\n",
        "    # We still need a label encoder to map these numeric values back to string labels\n",
        "    # for evaluation metrics and plotting (like confusion matrix).\n",
        "    # Fit LabelEncoder on the unique *valid* numeric labels and their corresponding strings\n",
        "    unique_numeric_labels = np.unique(y_valid[~np.isnan(y_valid)]) # Get unique non-NaN numeric labels\n",
        "    unique_string_labels = [valence_map.get(num_label, 'Unknown') for num_label in unique_numeric_labels] # Map back to strings\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    # Fit on the unique numeric labels directly, as they will be the classes for the classifier\n",
        "    label_encoder.fit(unique_numeric_labels)\n",
        "    print(f\"Label encoder fitted on unique valid numeric labels: {label_encoder.classes_}\")\n",
        "\n",
        "    # The y_valid is already in the correct numeric format for training RandomForestClassifier\n",
        "    # y_train will be y_valid\n",
        "\n",
        "\n",
        "    # --- Imputation (Apply imputation after feature extraction) ---\n",
        "    # Use the imputation function defined in cell 1703588a\n",
        "    print(\"\\nApplying Imputation on valid features...\")\n",
        "    if np.isnan(X_valid).any():\n",
        "        # Create an imputer and fit/transform\n",
        "        # Using SimpleImputer as fallback if MissForest is not available\n",
        "        imputer = SimpleImputer(strategy='mean') # Instantiate imputer\n",
        "        X_imputed = imputer.fit_transform(X_valid)\n",
        "        print(\"Imputation applied.\")\n",
        "    else:\n",
        "        X_imputed = X_valid # No imputation needed\n",
        "        imputer = None # No imputer was fitted\n",
        "\n",
        "\n",
        "    # --- 4. Train Model ---\n",
        "    print(\"\\nTraining Model...\")\n",
        "    try:\n",
        "        # Use the train_random_forest function defined in cell acb19d53\n",
        "        # Pass the imputed features and the valid numeric labels\n",
        "        training_result = train_random_forest(X_imputed, y_valid,\n",
        "                                                 n_estimators=args.trees,\n",
        "                                                 max_depth=args.max_depth)\n",
        "\n",
        "        model = training_result.get('model')\n",
        "        scaler = training_result.get('scaler') # Scaler from train_random_forest\n",
        "        # The mask returned by train_random_forest is based on NaNs in y_valid,\n",
        "        # but we already handled NaNs in y earlier using valid_label_mask.\n",
        "        # The scaler returned is fitted on the data *after* imputation within train_random_forest.\n",
        "\n",
        "        if model is None:\n",
        "            print(\"Model training failed.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Model trained successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during model training: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # --- 5. Save Model Artifacts ---\n",
        "    print(\"\\nSaving Model Artifacts...\")\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    try:\n",
        "        model_path = os.path.join(args.out_dir, \"valence_model.joblib\")\n",
        "        scaler_path = os.path.join(args.out_dir, \"valence_scaler.joblib\")\n",
        "        imputer_path = os.path.join(args.out_dir, \"valence_imputer.joblib\") # Save the imputer if used\n",
        "        label_encoder_path = os.path.join(args.out_dir, \"valence_label_encoder.joblib\")\n",
        "\n",
        "        joblib.dump(model, model_path)\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        if imputer: # Only save imputer if it was used\n",
        "             joblib.dump(imputer, imputer_path)\n",
        "        joblib.dump(label_encoder, label_encoder_path) # Save the label encoder\n",
        "\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "        print(f\"Scaler saved to {scaler_path}\")\n",
        "        if imputer:\n",
        "             print(f\"Imputer saved to {imputer_path}\")\n",
        "        print(f\"Label Encoder saved to {label_encoder_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model artifacts: {e}\")\n",
        "        # Continue execution even if saving fails, so evaluation can still run\n",
        "\n",
        "\n",
        "    # --- 6. Evaluate Model ---\n",
        "    # This section uses the test set (meld_test) if available globally.\n",
        "    print(\"\\nEvaluating Model on Test Set...\")\n",
        "    if 'meld_test' in globals() and meld_test is not None:\n",
        "        try:\n",
        "            # Build feature matrix for the test set\n",
        "            print(\"Building feature matrix for test set...\")\n",
        "            test_features_df = build_feature_matrix_valence(\n",
        "                meld_test, text_extractor, image_extractor, audio_extractor,\n",
        "                image_dir=None, # Specify if you have test image files\n",
        "                audio_dir=None, # Specify if you have test audio files\n",
        "                sentiment_lexicon=sentiment_lexicon,\n",
        "                # include_tda=args.tda, # REMOVED\n",
        "                # persim_obj=persim_obj # REMOVED\n",
        "            )\n",
        "            print(f\"Test feature matrix built: {test_features_df.shape}\")\n",
        "\n",
        "            # Separate features (X_test) and labels (y_test)\n",
        "            # Apply the same column filtering as training data\n",
        "            # Ensure 'valence_label_numeric' exists\n",
        "            if 'valence_label_numeric' in test_features_df.columns:\n",
        "                y_test_raw = test_features_df['valence_label_numeric'].values\n",
        "                y_test_string_labels_raw = test_features_df['valence_label'].values # Keep original string labels\n",
        "                X_test_raw = test_features_df[feature_cols].values # Use the same feature columns identified during training\n",
        "            else:\n",
        "                 print(\"Error: 'valence_label_numeric' column not found in test_features_df. Skipping evaluation.\")\n",
        "                 return {'model': model, 'scaler': scaler, 'imputer': imputer, 'label_encoder': label_encoder}\n",
        "\n",
        "\n",
        "            # Handle missing/invalid labels in test set\n",
        "            test_valid_mask = ~pd.isna(y_test_raw)\n",
        "            X_test_valid = X_test_raw[test_valid_mask]\n",
        "            y_test_valid = y_test_raw[test_valid_mask]\n",
        "            y_test_string_labels_valid = y_test_string_labels_raw[test_valid_mask] # Keep string labels for valid test data\n",
        "\n",
        "\n",
        "            # Apply the same imputer (fitted on training data) to the test features\n",
        "            print(\"Applying Imputation on valid test features...\")\n",
        "            # Check if imputer was fitted during training\n",
        "            if imputer:\n",
        "                if np.isnan(X_test_valid).any():\n",
        "                    X_test_imputed = imputer.transform(X_test_valid)\n",
        "                    print(\"Imputation applied to test features.\")\n",
        "                else:\n",
        "                    X_test_imputed = X_test_valid # No imputation needed for test set\n",
        "            else:\n",
        "                # If no imputer was fitted during training (because training data had no NaNs),\n",
        "                # but test data has NaNs, use a new imputer (fitted on test data only - WARNING: potential data leakage)\n",
        "                # A better approach is to fit imputer on training data and transform both train/test.\n",
        "                # Assuming the imputer from training is used if available. If not, fallback.\n",
        "                if np.isnan(X_test_valid).any():\n",
        "                    print(\"Warning: Training data had no NaNs, so no imputer was saved. Applying SimpleImputer (mean) on test features (potential data leakage).\")\n",
        "                    test_imputer = SimpleImputer(strategy='mean')\n",
        "                    X_test_imputed = test_imputer.fit_transform(X_test_valid)\n",
        "                else:\n",
        "                    X_test_imputed = X_test_valid\n",
        "\n",
        "\n",
        "            # Apply the same scaler (fitted on training data) to the test features\n",
        "            print(\"Applying Scaling on imputed test features...\")\n",
        "            if scaler:\n",
        "                 X_test_scaled = scaler.transform(X_test_imputed)\n",
        "                 print(\"Scaling applied to test features.\")\n",
        "            else:\n",
        "                 # This case should ideally not happen if scaling was applied during training\n",
        "                 print(\"Warning: Scaler not found from training. Skipping scaling on test features.\")\n",
        "                 X_test_scaled = X_test_imputed\n",
        "\n",
        "\n",
        "            # Make predictions\n",
        "            print(\"Making predictions on test set...\")\n",
        "            y_pred_valid = model.predict(X_test_scaled)\n",
        "\n",
        "            # Convert numeric predictions back to string labels for evaluation metrics\n",
        "            # Handle potential edge case where predictions are not in label_encoder.classes_\n",
        "            y_pred_string_labels_valid = []\n",
        "            for pred in y_pred_valid:\n",
        "                try:\n",
        "                    # Inverse transform might expect integer labels corresponding to fitted classes\n",
        "                    # Find the closest class in label_encoder.classes_\n",
        "                    closest_class_index = np.argmin(np.abs(label_encoder.classes_ - pred))\n",
        "                    predicted_numeric_label = label_encoder.classes_[closest_class_index]\n",
        "                    # Inverse transform the matched numeric label\n",
        "                    predicted_string = label_encoder.inverse_transform([predicted_numeric_label])[0]\n",
        "                    y_pred_string_labels_valid.append(predicted_string)\n",
        "                except Exception as e:\n",
        "                    # Fallback if inverse transform fails or prediction is unexpected\n",
        "                    print(f\"Warning: Could not inverse transform prediction {pred}: {e}. Assigning 'Unknown'.\")\n",
        "                    y_pred_string_labels_valid.append('Unknown')\n",
        "\n",
        "            # Ensure y_test_string_labels_valid and y_pred_string_labels_valid are lists or arrays of strings\n",
        "            y_test_string_labels_valid = y_test_string_labels_valid.tolist() if isinstance(y_test_string_labels_valid, np.ndarray) else y_test_string_labels_valid\n",
        "            y_pred_string_labels_valid = y_pred_string_labels_valid # Already a list\n",
        "\n",
        "            # Evaluate based on the task type (classification for valence 1/-1/0)\n",
        "            # Ensure labels used for metrics are consistent (string or numeric)\n",
        "            # Let's use numeric labels for sklearn metrics that expect numeric input\n",
        "            # and string labels for plotting confusion matrix with meaningful labels.\n",
        "\n",
        "            print(\"\\n--- Evaluation Metrics (Valence) ---\")\n",
        "            # Ensure y_test_valid and y_pred_valid are in a format compatible with metrics\n",
        "            # They should be arrays of numeric values (1, -1, 0)\n",
        "            if model.__class__.__name__ == 'RandomForestClassifier':\n",
        "                 # Classification metrics\n",
        "                 accuracy = accuracy_score(y_test_valid, y_pred_valid)\n",
        "                 # Use average='weighted' for multi-class with potential imbalance\n",
        "                 f1 = f1_score(y_test_valid, y_pred_valid, average='weighted', zero_division=0)\n",
        "                 precision = precision_score(y_test_valid, y_pred_valid, average='weighted', zero_division=0)\n",
        "                 recall = recall_score(y_test_valid, y_pred_valid, average='weighted', zero_division=0)\n",
        "\n",
        "                 print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                 print(f\"F1 Score (weighted): {f1:.4f}\")\n",
        "                 print(f\"Precision (weighted): {precision:.4f}\")\n",
        "                 print(f\"Recall (weighted): {recall:.4f}\")\n",
        "\n",
        "                 # Store evaluation results globally (optional, for external access if needed)\n",
        "                 evaluation_results_valence = {\n",
        "                     'y_true': y_test_string_labels_valid, # Store string labels for plotting\n",
        "                     'y_pred': y_pred_string_labels_valid, # Store string labels for plotting\n",
        "                     'class_labels': label_encoder.inverse_transform(label_encoder.classes_).tolist(), # Get sorted string class labels\n",
        "                     'accuracy': accuracy,\n",
        "                     'f1_weighted': f1,\n",
        "                     'precision_weighted': precision,\n",
        "                     'recall_weighted': recall\n",
        "                 }\n",
        "\n",
        "                 # Plot Confusion Matrix\n",
        "                 print(\"\\nPlotting Confusion Matrix...\")\n",
        "                 # Use string labels for plotting for clarity\n",
        "                 plot_confusion_matrix(\n",
        "                     evaluation_results_valence['y_true'],\n",
        "                     evaluation_results_valence['y_pred'],\n",
        "                     evaluation_results_valence['class_labels'],\n",
        "                     save_path=os.path.join(args.out_dir, \"valence_confusion_matrix.png\")\n",
        "                 )\n",
        "\n",
        "\n",
        "            elif model.__class__.__name__ == 'RandomForestRegressor':\n",
        "                 # Regression metrics (if using regression for valence score directly)\n",
        "                 mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "                 print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "                 # Store regression evaluation results\n",
        "                 evaluation_results_valence = {\n",
        "                     'y_true': y_test_valid,\n",
        "                     'y_pred': y_pred_valid,\n",
        "                     'mae': mae\n",
        "                 }\n",
        "\n",
        "            print(\"------------------------------\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during model evaluation: {e}\")\n",
        "            # Continue, but return the trained model package without evaluation results\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Test set (meld_test) not found or is None. Skipping evaluation.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Training Pipeline Complete ---\")\n",
        "\n",
        "    # Return the trained model package\n",
        "    return {'model': model, 'scaler': scaler, 'imputer': imputer, 'label_encoder': label_encoder}\n",
        "\n",
        "print(\"run_training_pipeline function defined.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_training_pipeline function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71ef44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd23b43f-31f9-4727-ef96-ccd476fc34cb"
      },
      "source": [
        "# This block contains the configuration and execution part of cell Vn1D5VmzP0BY.\n",
        "# It assumes the run_training_pipeline function is already defined in the previous cell output.\n",
        "\n",
        "# Define a simple class to hold the arguments (configuration)\n",
        "class ConfigArgs:\n",
        "    \"\"\"Simple class to hold configuration parameters.\"\"\"\n",
        "    def __init__(self, manifest, out_dir=\"./models\", bert_model=DEFAULT_BERT, tda=False, trees=300, max_depth=None):\n",
        "        self.manifest = manifest\n",
        "        self.out_dir = out_dir\n",
        "        self.bert_model = bert_model\n",
        "        self.tda = tda\n",
        "        self.trees = trees\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "# --- Set your configuration here ---\n",
        "# Create an instance of the ConfigArgs class with your desired parameters.\n",
        "# The 'manifest' argument is required. We will create a manifest\n",
        "# from the loaded MELD training data (`meld_train`) for demonstration.\n",
        "\n",
        "# Ensure the dummy manifest file exists (re-create it if necessary)\n",
        "# Create a manifest DataFrame that includes columns needed by build_feature_matrix_valence\n",
        "# Use the loaded meld_train DataFrame (from cell 89StvGbdyzx8)\n",
        "dummy_manifest_path = \"/content/meld_train_manifest_full.csv\"\n",
        "\n",
        "# Check if meld_train DataFrame is available globally and is not None\n",
        "if 'meld_train' in globals() and meld_train is not None:\n",
        "    # Select and rename columns needed by build_feature_matrix_valence\n",
        "    # Columns needed: 'text', 'label', 'valence_label', 'Dialogue_ID', 'Utterance_ID', 'Speaker'\n",
        "    # meld_train has: 'Utterance', 'Emotion', 'Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Speaker' (and others)\n",
        "    # Ensure all required columns exist in meld_train before selecting\n",
        "    required_meld_cols = ['Utterance', 'Emotion', 'Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Speaker']\n",
        "    available_meld_cols = [col for col in required_meld_cols if col in meld_train.columns]\n",
        "\n",
        "    # Create manifest_df using only available required columns\n",
        "    manifest_df = meld_train[available_meld_cols].copy()\n",
        "\n",
        "    # Rename columns to match build_feature_matrix_valence expectations\n",
        "    rename_map = {'Utterance': 'text', 'Emotion': 'label', 'Sentiment': 'valence_label'}\n",
        "    # Only apply renames for columns that exist and are in the map\n",
        "    cols_to_rename = {k: v for k, v in rename_map.items() if k in manifest_df.columns}\n",
        "    manifest_df = manifest_df.rename(columns=cols_to_rename)\n",
        "\n",
        "    # Add any missing expected columns as None to ensure consistency for build_feature_matrix_valence\n",
        "    expected_manifest_cols = ['text', 'label', 'valence_label', 'Dialogue_ID', 'Utterance_ID', 'Speaker', 'image_filename', 'audio_filename'] # Added image/audio filenames\n",
        "    for col in expected_manifest_cols:\n",
        "         if col not in manifest_df.columns:\n",
        "              manifest_df[col] = None # Add missing columns with None\n",
        "\n",
        "    try:\n",
        "        manifest_df.to_csv(dummy_manifest_path, index=False)\n",
        "        print(f\"Created/Updated dummy manifest at: {dummy_manifest_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error saving dummy manifest to {dummy_manifest_path}: {e}. Cannot proceed.\")\n",
        "         dummy_manifest_path = None # Set to None if saving fails\n",
        "else:\n",
        "    print(\"Warning: meld_train DataFrame not found or is None. Cannot create dummy manifest.\")\n",
        "    dummy_manifest_path = None\n",
        "\n",
        "\n",
        "# Create an args object with the desired configuration\n",
        "# Only proceed if the dummy manifest path was successfully created\n",
        "if dummy_manifest_path is not None:\n",
        "    # Ensure _HAS_TDA is defined globally (it's from cell 6iJgCVCX4Jl8 if TDA libs were imported)\n",
        "    # Check if _HAS_TDA is defined globally, otherwise default to False\n",
        "    _HAS_TDA = globals().get('_HAS_TDA', False)\n",
        "\n",
        "\n",
        "    args_valence_train = ConfigArgs(\n",
        "        manifest=dummy_manifest_path, # Use the path to the created manifest CSV\n",
        "        out_dir=\"./models_valence\", # Output directory for the model artifacts\n",
        "        bert_model=DEFAULT_BERT, # Use the default BERT model defined earlier (in cell 6iJgCVCX4Jl8)\n",
        "        tda=_HAS_TDA, # Include TDA features if the libraries were successfully imported (_HAS_TDA is global)\n",
        "        trees=300, # Number of trees for Random Forest\n",
        "        max_depth=None # Max depth for Random Forest (None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.)\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Running training pipeline with configuration: ---\")\n",
        "    print(f\"  Manifest: {args_valence_train.manifest}\")\n",
        "    print(f\"  Output Directory: {args_valence_train.out_dir}\")\n",
        "    print(f\"  BERT Model: {args_valence_train.bert_model}\")\n",
        "    print(f\"  Use TDA: {args_valence_train.tda}\")\n",
        "    print(f\"  Random Forest Trees: {args_valence_train.trees}\")\n",
        "    print(f\"  Random Forest Max Depth: {args_valence_train.max_depth}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "    # Call the main training pipeline function with the configuration object\n",
        "    print(\"\\nCalling run_training_pipeline...\")\n",
        "    # The run_training_pipeline function now handles both training and evaluation internally.\n",
        "    # It returns the trained model package or None if training failed.\n",
        "    trained_model_package = run_training_pipeline(args_valence_train)\n",
        "\n",
        "    if trained_model_package and trained_model_package.get('model') is not None: # Use .get() for safety\n",
        "         print(\"\\n--- Training and Evaluation pipeline execution complete. ---\")\n",
        "         print(\"Model trained and evaluated successfully.\")\n",
        "         # trained_model_package contains the trained model, scaler, imputer, label_encoder\n",
        "    else:\n",
        "         print(\"\\n--- Training and Evaluation pipeline execution complete. ---\")\n",
        "         print(\"Model training failed. Evaluation skipped.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- Training and Evaluation pipeline execution skipped ---\")\n",
        "    print(\"Reason: Dummy manifest file could not be created.\")\n",
        "    print(\"------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created/Updated dummy manifest at: /content/meld_train_manifest_full.csv\n",
            "\n",
            "--- Running training pipeline with configuration: ---\n",
            "  Manifest: /content/meld_train_manifest_full.csv\n",
            "  Output Directory: ./models_valence\n",
            "  BERT Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "  Use TDA: False\n",
            "  Random Forest Trees: 300\n",
            "  Random Forest Max Depth: None\n",
            "-------------------------------------------------\n",
            "\n",
            "Calling run_training_pipeline...\n",
            "\n",
            "--- Running Training Pipeline ---\n",
            "Manifest loaded: (9989, 8)\n",
            "\n",
            "Initializing Feature Extractors...\n",
            "An unexpected error occurred during extractor initialization: cannot access local variable 'torchvision' where it is not associated with a value\n",
            "\n",
            "--- Training and Evaluation pipeline execution complete. ---\n",
            "Model training failed. Evaluation skipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21d6033c"
      },
      "source": [
        "# This block contains the configuration and execution part of cell Vn1D5VmzP0BY.\n",
        "# It assumes the run_training_pipeline function is already defined in the previous cell output.\n",
        "\n",
        "# Define a simple class to hold the arguments (configuration)\n",
        "class ConfigArgs:\n",
        "    \"\"\"Simple class to hold configuration parameters.\"\"\"\n",
        "    def __init__(self, manifest, out_dir=\"./models\", bert_model=DEFAULT_BERT, tda=False, trees=300, max_depth=None):\n",
        "        self.manifest = manifest\n",
        "        self.out_dir = out_dir\n",
        "        self.bert_model = bert_model\n",
        "        self.tda = tda\n",
        "        self.trees = trees\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "# --- Set your configuration here ---\n",
        "# Create an instance of the ConfigArgs class with your desired parameters.\n",
        "# The 'manifest' argument is required. We will create a manifest\n",
        "# from the loaded MELD training data (`meld_train`) for demonstration.\n",
        "\n",
        "# Ensure the dummy manifest file exists (re-create it if necessary)\n",
        "# Create a manifest DataFrame that includes columns needed by build_feature_matrix_valence\n",
        "# Use the loaded meld_train DataFrame (from cell 89StvGbdyzx8)\n",
        "dummy_manifest_path = \"/content/meld_train_manifest_full.csv\"\n",
        "\n",
        "# Check if meld_train DataFrame is available globally and is not None\n",
        "if 'meld_train' in globals() and meld_train is not None:\n",
        "    # Select and rename columns needed by build_feature_matrix_valence\n",
        "    # Columns needed: 'text', 'label', 'valence_label', 'Dialogue_ID', 'Utterance_ID', 'Speaker'\n",
        "    # meld_train has: 'Utterance', 'Emotion', 'Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Speaker' (and others)\n",
        "    # Ensure all required columns exist in meld_train before selecting\n",
        "    required_meld_cols = ['Utterance', 'Emotion', 'Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Speaker']\n",
        "    available_meld_cols = [col for col in required_meld_cols if col in meld_train.columns]\n",
        "\n",
        "    # Create manifest_df using only available required columns\n",
        "    manifest_df = meld_train[available_meld_cols].copy()\n",
        "\n",
        "    # Rename columns to match build_feature_matrix_valence expectations\n",
        "    rename_map = {'Utterance': 'text', 'Emotion': 'label', 'Sentiment': 'valence_label'}\n",
        "    # Only apply renames for columns that exist and are in the map\n",
        "    cols_to_rename = {k: v for k, v in rename_map.items() if k in manifest_df.columns}\n",
        "    manifest_df = manifest_df.rename(columns=cols_to_rename)\n",
        "\n",
        "    # Add any missing expected columns as None to ensure consistency for build_feature_matrix_valence\n",
        "    expected_manifest_cols = ['text', 'label', 'valence_label', 'Dialogue_ID', 'Utterance_ID', 'Speaker', 'image_filename', 'audio_filename'] # Added image/audio filenames\n",
        "    for col in expected_manifest_cols:\n",
        "         if col not in manifest_df.columns:\n",
        "              manifest_df[col] = None # Add missing columns with None\n",
        "\n",
        "    try:\n",
        "        manifest_df.to_csv(dummy_manifest_path, index=False)\n",
        "        print(f\"Created/Updated dummy manifest at: {dummy_manifest_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error saving dummy manifest to {dummy_manifest_path}: {e}. Cannot proceed.\")\n",
        "         dummy_manifest_path = None # Set to None if saving fails\n",
        "else:\n",
        "    print(\"Warning: meld_train DataFrame not found or is None. Cannot create dummy manifest.\")\n",
        "    dummy_manifest_path = None\n",
        "\n",
        "\n",
        "# Create an args object with the desired configuration\n",
        "# Only proceed if the dummy manifest path was successfully created\n",
        "if dummy_manifest_path is not None:\n",
        "    # Ensure _HAS_TDA is defined globally (it's from cell 6iJgCVCX4Jl8 if TDA libs were imported)\n",
        "    # Check if _HAS_TDA is defined globally, otherwise default to False - REMOVED\n",
        "    # _HAS_TDA = globals().get('_HAS_TDA', False) # REMOVED\n",
        "\n",
        "\n",
        "    args_valence_train = ConfigArgs(\n",
        "        manifest=dummy_manifest_path, # Use the path to the created manifest CSV\n",
        "        out_dir=\"./models_valence\", # Output directory for the model artifacts\n",
        "        bert_model=DEFAULT_BERT, # Use the default BERT model defined earlier (in cell 6iJgCVCX4Jl8)\n",
        "        tda=False, # Include TDA features if the libraries were successfully imported (_HAS_TDA is global) - MODIFIED to False\n",
        "        trees=300, # Number of trees for Random Forest\n",
        "        max_depth=None # Max depth for Random Forest (None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.)\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Running training pipeline with configuration: ---\")\n",
        "    print(f\"  Manifest: {args_valence_train.manifest}\")\n",
        "    print(f\"  Output Directory: {args_valence_train.out_dir}\")\n",
        "    print(f\"  BERT Model: {args_valence_train.bert_model}\")\n",
        "    print(f\"  Use TDA: {args_valence_train.tda}\")\n",
        "    print(f\"  Random Forest Trees: {args_valence_train.trees}\")\n",
        "    print(f\"  Random Forest Max Depth: {args_valence_train.max_depth}\")\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "    # Call the main training pipeline function with the configuration object\n",
        "    print(\"\\nCalling run_training_pipeline...\")\n",
        "    # The run_training_pipeline function now handles both training and evaluation internally.\n",
        "    # It returns the trained model package or None if training failed.\n",
        "    trained_model_package = run_training_pipeline(args_valence_train)\n",
        "\n",
        "    if trained_model_package and trained_model_package.get('model') is not None: # Use .get() for safety\n",
        "         print(\"\\n--- Training and Evaluation pipeline execution complete. ---\")\n",
        "         print(\"Model trained and evaluated successfully.\")\n",
        "         # trained_model_package contains the trained model, scaler, imputer, label_encoder\n",
        "    else:\n",
        "         print(\"\\n--- Training and Evaluation pipeline execution complete. ---\")\n",
        "         print(\"Model training failed. Evaluation skipped.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- Training and Evaluation pipeline execution skipped ---\")\n",
        "    print(\"Reason: Dummy manifest file could not be created.\")\n",
        "    print(\"------------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USeDlns18i4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4413b5-eea4-432d-cbca-c2abe60ab020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 5: Imputation function defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell USeDlns18i4Z: Stage 5 - Imputation Function\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import numpy as np\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# try: from missingpy import MissForest # Import if available\n",
        "# except ImportError: pass # Handle import error\n",
        "\n",
        "def impute_missing(X):\n",
        "    \"\"\"\n",
        "    Impute missing values in feature matrix X.\n",
        "    Uses MissForest if available, otherwise SimpleImputer (mean).\n",
        "    \"\"\"\n",
        "    print(\"Applying imputation...\")\n",
        "    # Check if MissForest is available globally\n",
        "    if '_HAS_MISSFOREST' in globals() and _HAS_MISSFOREST:\n",
        "        try:\n",
        "            imputer = MissForest()\n",
        "            X_imputed = imputer.fit_transform(X)\n",
        "            print(\"✓ MissForest imputation applied.\")\n",
        "            return X_imputed\n",
        "        except Exception as e:\n",
        "            print(f\"Error using MissForest: {e}. Falling back to SimpleImputer.\")\n",
        "            # Fallback to SimpleImputer if MissForest fails\n",
        "            imp = SimpleImputer(strategy='mean')\n",
        "            X_imputed = imp.fit_transform(X)\n",
        "            print(\"✓ SimpleImputer (mean) applied.\")\n",
        "            return X_imputed\n",
        "    else:\n",
        "        imp = SimpleImputer(strategy='mean')\n",
        "        X_imputed = imp.fit_transform(X)\n",
        "        print(\"✓ SimpleImputer (mean) applied.\")\n",
        "        return X_imputed\n",
        "\n",
        "print(\"\\nStage 5: Imputation function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jub6R6AP_r3D"
      },
      "source": [
        "###  Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JclnoRXSR-uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7400cec8-64f8-4120-d055-669c9d575ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 5: Training function defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell JclnoRXSR-uu: Stage 5 - Training Function\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import numpy as np, pandas as pd\n",
        "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.impute import SimpleImputer # Only needed if fallback imputation is here\n",
        "\n",
        "def train_random_forest(X, y, n_estimators=100, max_depth=None):\n",
        "    \"\"\"\n",
        "    Train a Random Forest (classifier or regressor) with imputation and scaling.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Feature matrix (may contain NaNs).\n",
        "        y (np.ndarray): Target labels (numerical, may contain NaNs).\n",
        "        n_estimators (int): Number of trees.\n",
        "        max_depth (int or None): Maximum tree depth.\n",
        "\n",
        "    Returns:\n",
        "        dict: Trained model, scaler, and label mask. (Imputer assumed to be handled before calling)\n",
        "    \"\"\"\n",
        "    print(\"Starting Random Forest training...\")\n",
        "    # 1. Filter out samples with missing labels\n",
        "    mask = ~pd.isna(y)\n",
        "    if mask.sum() == 0:\n",
        "        print(\"No valid labels found. Cannot train model.\")\n",
        "        return {'model': None, 'scaler': None, 'mask': mask}\n",
        "\n",
        "    X_train, y_train = X[mask], y[mask]\n",
        "\n",
        "    # 2. Decide task type (classification vs regression)\n",
        "    # Check if labels are integers (implies classification) or floats (implies regression or potentially classification)\n",
        "    # A simple check is if all non-NaN values are integers\n",
        "    if np.issubdtype(y_train.dtype, np.number) and np.all(np.mod(y_train[~np.isnan(y_train)], 1) == 0):\n",
        "        y_train = y_train.astype(int)\n",
        "        model_type = 'classifier'\n",
        "        print(\"Training a RandomForestClassifier.\")\n",
        "    else:\n",
        "        model_type = 'regressor'\n",
        "        print(\"Training a RandomForestRegressor.\")\n",
        "\n",
        "\n",
        "    # 3. Impute missing values (Check and fallback imputation moved here)\n",
        "    # Ensure X_train does not have NaNs here.\n",
        "    # If imputation is expected within this function, move impute_missing call here.\n",
        "    # Based on the pipeline structure, imputation happens after feature building but before training.\n",
        "    # So X should be imputed before being passed to this function.\n",
        "    # Let's add a check and basic imputation fallback just in case.\n",
        "    if np.isnan(X_train).any():\n",
        "        print(\"Warning: NaNs found in training features before model training. Applying SimpleImputer (mean).\")\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_train_imp = imputer.fit_transform(X_train)\n",
        "        # Note: The imputer fitted here is not returned by this function.\n",
        "        # If you need to use the *same* imputer for test data, imputation should\n",
        "        # be done *before* calling train_random_forest, and the imputer should be\n",
        "        # returned by the function that performs imputation (like impute_missing).\n",
        "        # Assuming impute_missing is called before this function.\n",
        "    else:\n",
        "        X_train_imp = X_train # No imputation needed if no NaNs\n",
        "\n",
        "    # 4. Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_imp)\n",
        "    print(\"Features scaled.\")\n",
        "\n",
        "\n",
        "    # 5. Train model\n",
        "    if model_type == 'classifier':\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators, max_depth=max_depth,\n",
        "            random_state=42, n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators, max_depth=max_depth,\n",
        "            random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "    print(f\"Training model with {n_estimators} trees and max_depth={max_depth}...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # Return scaler and imputer used within this function if imputation was done here.\n",
        "    # If imputation is done *before* calling this function, return the ones used there.\n",
        "    # Assuming imputation happens before, so imputer is not returned from here.\n",
        "    return {'model': model, 'scaler': scaler, 'mask': mask} # Removed imputer from return\n",
        "\n",
        "\n",
        "print(\"Stage 5: Training function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsQIqdUGPxDs"
      },
      "source": [
        "### CLI & Orchestration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn1D5VmzP0BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d5e3db-af92-4772-b0dc-8a73f96bd554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration class ConfigArgs defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell Vn1D5VmzP0BY: Stage 5: Fusion & Robustness (Orchestration - Configuration)\n",
        "\n",
        "# This cell defines the configuration class for the pipeline.\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1, but included for clarity)\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import argparse # Keep import if you want to potentially use argparse structure elsewhere\n",
        "\n",
        "\n",
        "# Define a simple class to hold the arguments (configuration)\n",
        "class ConfigArgs:\n",
        "    \"\"\"Simple class to hold configuration parameters.\"\"\"\n",
        "    def __init__(self, manifest, out_dir=\"./models\", bert_model=DEFAULT_BERT, tda=False, trees=300, max_depth=None):\n",
        "        self.manifest = manifest\n",
        "        self.out_dir = out_dir\n",
        "        self.bert_model = bert_model\n",
        "        self.tda = tda\n",
        "        self.trees = trees\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "# The pipeline execution logic (manifest creation and calling run_training_pipeline)\n",
        "# has been moved to a separate cell that will be executed after the\n",
        "# run_training_pipeline function is defined.\n",
        "\n",
        "print(\"Configuration class ConfigArgs defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7d56de"
      },
      "source": [
        "## **Stage 5: Random Forest Heads**\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e08cead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028664d9-b08a-4ba6-97b0-6ac535778343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 6: Model Evaluation stage defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 8e08cead: Stage 6 - Model Evaluation\n",
        "\n",
        "# This cell is for evaluating the trained model on the test set.\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import pandas as pd, numpy as np, os, joblib\n",
        "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "# Ensure build_feature_matrix_valence, preprocess_image, preprocess_audio,\n",
        "# TextFeatureExtractor, ImageFeatureExtractor, AudioFeatureExtractorValence,\n",
        "# sentiment_lexicon, plot_confusion_matrix are defined globally.\n",
        "# Ensure meld_test DataFrame is loaded globally.\n",
        "\n",
        "\n",
        "# Global variable to store evaluation results (e.g., for plotting confusion matrix)\n",
        "evaluation_results_valence = None\n",
        "\n",
        "# The evaluation logic is now integrated into the run_training_pipeline function\n",
        "# in the Orchestration cell (Vn1D5VmzP0BY). This cell now primarily serves\n",
        "# as a placeholder for the evaluation stage and the global results variable.\n",
        "\n",
        "print(\"Stage 6: Model Evaluation stage defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzGO5etki5iU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoS40yGljBil"
      },
      "source": [
        "## **Stage 7: Outputs**\n",
        "---\n",
        "*   Polarity Mapping (Positive/Neutral/Negative)\n",
        "*   Explainability(SHAP/Lime, Confusion Matrices, Persistence Diagrams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fdd07f"
      },
      "source": [
        "### *Confusion Matrix*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c63e159c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1adf9b-0082-4fb7-ce19-fd5b7b4365de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stage 7: Confusion Matrix plotting function defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell c63e159c: Stage 7 - Outputs (Confusion Matrix Plotting)\n",
        "\n",
        "# This cell contains the function to plot the confusion matrix.\n",
        "\n",
        "# Ensure necessary imports are available (should be covered by Stage 1)\n",
        "# import matplotlib.pyplot as plt, seaborn as sns, pandas as pd, numpy as np, os\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# Ensure evaluation_results_valence is available globally if used outside the function\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_labels, save_path=None):\n",
        "    \"\"\"\n",
        "    Calculates and plots the confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): True labels (numerical or string, consistent with y_pred).\n",
        "        y_pred (np.ndarray): Predicted labels (numerical or string, consistent with y_true).\n",
        "        class_labels (list or np.ndarray): List of class labels corresponding to the numerical encoding.\n",
        "        save_path (str, optional): Path to save the plot. If None, the plot is displayed.\n",
        "                                    Defaults to None.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "\n",
        "    # Ensure we have valid inputs\n",
        "    if y_true is None or y_pred is None or class_labels is None:\n",
        "        print(\"Error: True labels, predicted labels, or class labels are missing.\")\n",
        "        return\n",
        "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_true) != len(y_pred):\n",
        "        print(\"Error: True and predicted labels are empty or have inconsistent lengths.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Compute the confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        # Create a DataFrame for better visualization\n",
        "        # Ensure class_labels is a list for DataFrame index/columns\n",
        "        cm_df = pd.DataFrame(cm, index=list(class_labels), columns=list(class_labels))\n",
        "\n",
        "        # Plot the confusion matrix\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        # Save or display the plot\n",
        "        if save_path:\n",
        "            try:\n",
        "                # Ensure directory exists\n",
        "                save_dir = os.path.dirname(save_path)\n",
        "                if save_dir and not os.path.exists(save_dir):\n",
        "                    os.makedirs(save_dir)\n",
        "                plt.savefig(save_path, bbox_inches='tight')\n",
        "                print(f\"Saved confusion matrix plot to {save_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving confusion matrix plot to {save_path}: {e}\")\n",
        "                plt.show() # Display if saving fails\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error computing confusion matrix: {e}\")\n",
        "        print(\"This might be due to inconsistent labels between true values and predictions, or prediction issues.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during confusion matrix plotting: {e}\")\n",
        "\n",
        "# Note: The plotting function is defined here, but it is called from\n",
        "# the run_training_pipeline function in the Orchestration cell (Vn1D5VmzP0BY)\n",
        "# after evaluation is complete.\n",
        "\n",
        "print(\"\\nStage 7: Confusion Matrix plotting function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4vU31kOKs8Ne"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Za82e8vjpaiU",
        "jub6R6AP_r3D",
        "QsQIqdUGPxDs",
        "9c7d56de"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}